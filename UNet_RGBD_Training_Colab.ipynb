{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# U-Net RGB-D Training on Google Colab (Optimized)\n",
                "\n",
                "This notebook trains a U-Net semantic segmentation model on RGB-D images using GPU acceleration.\n",
                "\n",
                "**OPTIMIZED FOR SPEED:**\n",
                "1. âœ… **Local Data Copying:** copies dataset from Drive to VM to fix slow I/O\n",
                "2. âœ… **Configurable:** Choose between 'Fast' (128x128) or 'High Quality' (256x256)\n",
                "\n",
                "**Steps:**\n",
                "1. âœ… Enable GPU: Runtime â†’ Change runtime type â†’ GPU (T4)\n",
                "2. âœ… Upload Cityscapes dataset to Google Drive\n",
                "3. âœ… Run all cells\n",
                "4. âœ… Download trained model\n",
                "\n",
                "**Expected Training Time:** ~15-20 minutes on GPU"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "import torch\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"CUDA version: {torch.version.cuda}\")\n",
                "else:\n",
                "    print(\"âš ï¸ WARNING: No GPU detected! Enable GPU in Runtime â†’ Change runtime type\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q albumentations tqdm matplotlib scikit-learn opencv-python"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Mount & Copy Dataset (Critical for Speed)\n",
                "**Crucial Step:** We copy files from Google Drive to the local Colab VM. This prevents slow network I/O during training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "import shutil\n",
                "import os\n",
                "import time\n",
                "\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Source path in Drive\n",
                "DRIVE_PATH =('/content/drive/MyDrive/Cityscapes')\n",
                "# Destination path in local VM\n",
                "LOCAL_PATH = '/content/Cityscapes'\n",
                "\n",
                "if os.path.exists(DRIVE_PATH):\n",
                "    if not os.path.exists(LOCAL_PATH):\n",
                "        print(f\"ðŸš€ Copying dataset from Drive to local VM for speed... (This takes 2-3 mins)\")\n",
                "        start_time = time.time()\n",
                "        \n",
                "        # Copy instruction\n",
                "        !cp -r \"{DRIVE_PATH}\" \"/content/\"\n",
                "        \n",
                "        print(f\"âœ“ Copy complete! Time: {time.time() - start_time:.1f}s\")\n",
                "    else:\n",
                "        print(\"âœ“ Dataset already exists locally!\")\n",
                "    \n",
                "    # Set the dataset path for training\n",
                "    DATASET_PATH = LOCAL_PATH\n",
                "    print(f\"âœ“ Training will use: {DATASET_PATH}\")\n",
                "    !ls -lh {DATASET_PATH}\n",
                "    \n",
                "else:\n",
                "    print(f\"âŒ Dataset not found at: {DRIVE_PATH}\")\n",
                "    print(\"Please upload Cityscapes to Google Drive (MyDrive/Cityscapes)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Configuration (Quality vs Speed)\n",
                "Select your training mode here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# MODE SELECTION\n",
                "# 'fast' = 128x128 images, smaller model (Same as your CPU training)\n",
                "# 'quality' = 256x256 images, larger model (Better results, slower)\n",
                "\n",
                "TRAINING_MODE = 'fast'  # Change to 'quality' if you want better results\n",
                "\n",
                "if TRAINING_MODE == 'fast':\n",
                "    config = {\n",
                "        'in_channels': 4,\n",
                "        'num_classes': 34,\n",
                "        'base_channels': 32,    # Smaller model\n",
                "        'batch_size': 8,        # Larger batch size for speed\n",
                "        'num_epochs': 20,\n",
                "        'learning_rate': 1e-4,\n",
                "        'img_size': 128,        # Smaller images\n",
                "        'num_workers': 2,\n",
                "        'device': 'cuda'\n",
                "    }\n",
                "    print(\"ðŸš€ Mode: FAST (128x128, Small Model)\")\n",
                "else:\n",
                "    config = {\n",
                "        'in_channels': 4,\n",
                "        'num_classes': 34,\n",
                "        'base_channels': 64,    # Larger model\n",
                "        'batch_size': 4,        # Moderate batch size\n",
                "        'num_epochs': 20,\n",
                "        'learning_rate': 1e-4,\n",
                "        'img_size': 256,        # Larger images\n",
                "        'num_workers': 2,\n",
                "        'device': 'cuda'\n",
                "    }\n",
                "    print(\"âœ¨ Mode: QUALITY (256x256, Large Model)\")\n",
                "\n",
                "# Paths\n",
                "config['images_base'] = f'{DATASET_PATH}/leftImg8bit_trainvaltest/leftImg8bit'\n",
                "config['labels_base'] = f'{DATASET_PATH}/gtFine_trainvaltest/gtFine'\n",
                "config['depth_base'] = f'{DATASET_PATH}/disparity_trainvaltest/disparity'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Define U-Net Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "\n",
                "class DoubleConv(nn.Module):\n",
                "    \"\"\"Double Convolution block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(DoubleConv, self).__init__()\n",
                "        self.double_conv = nn.Sequential(\n",
                "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
                "            nn.BatchNorm2d(out_channels),\n",
                "            nn.ReLU(inplace=True)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.double_conv(x)\n",
                "\n",
                "class DownBlock(nn.Module):\n",
                "    \"\"\"Encoder block: MaxPool -> DoubleConv\"\"\"\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(DownBlock, self).__init__()\n",
                "        self.maxpool_conv = nn.Sequential(\n",
                "            nn.MaxPool2d(2),\n",
                "            DoubleConv(in_channels, out_channels)\n",
                "        )\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.maxpool_conv(x)\n",
                "\n",
                "class UpBlock(nn.Module):\n",
                "    \"\"\"Decoder block: Upsample -> Concat -> DoubleConv\"\"\"\n",
                "    def __init__(self, in_channels, out_channels):\n",
                "        super(UpBlock, self).__init__()\n",
                "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
                "        self.conv = DoubleConv(in_channels, out_channels)\n",
                "    \n",
                "    def forward(self, x1, x2):\n",
                "        x1 = self.up(x1)\n",
                "        x = torch.cat([x2, x1], dim=1)\n",
                "        return self.conv(x)\n",
                "\n",
                "class UNetRGBD(nn.Module):\n",
                "    \"\"\"U-Net for RGB-D Semantic Segmentation\"\"\"\n",
                "    def __init__(self, in_channels=4, num_classes=34, base_channels=64, dropout_p=0.5):\n",
                "        super(UNetRGBD, self).__init__()\n",
                "        \n",
                "        # Encoder\n",
                "        self.inc = DoubleConv(in_channels, base_channels)\n",
                "        self.down1 = DownBlock(base_channels, base_channels * 2)\n",
                "        self.down2 = DownBlock(base_channels * 2, base_channels * 4)\n",
                "        self.down3 = DownBlock(base_channels * 4, base_channels * 8)\n",
                "        \n",
                "        # Bottleneck\n",
                "        self.down4 = DownBlock(base_channels * 8, base_channels * 16)\n",
                "        self.dropout = nn.Dropout2d(p=dropout_p)\n",
                "        \n",
                "        # Decoder\n",
                "        self.up1 = UpBlock(base_channels * 16, base_channels * 8)\n",
                "        self.up2 = UpBlock(base_channels * 8, base_channels * 4)\n",
                "        self.up3 = UpBlock(base_channels * 4, base_channels * 2)\n",
                "        self.up4 = UpBlock(base_channels * 2, base_channels)\n",
                "        \n",
                "        # Output\n",
                "        self.outc = nn.Conv2d(base_channels, num_classes, kernel_size=1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        # Encoder\n",
                "        x1 = self.inc(x)\n",
                "        x2 = self.down1(x1)\n",
                "        x3 = self.down2(x2)\n",
                "        x4 = self.down3(x3)\n",
                "        x5 = self.down4(x4)\n",
                "        x5 = self.dropout(x5)\n",
                "        \n",
                "        # Decoder\n",
                "        x = self.up1(x5, x4)\n",
                "        x = self.up2(x, x3)\n",
                "        x = self.up3(x, x2)\n",
                "        x = self.up4(x, x1)\n",
                "        logits = self.outc(x)\n",
                "        return logits\n",
                "    \n",
                "    def get_num_params(self):\n",
                "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
                "\n",
                "# Test model\n",
                "model = UNetRGBD(in_channels=4, num_classes=34, base_channels=config['base_channels'])\n",
                "print(f\"âœ“ Model created: {model.get_num_params():,} parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Create RGB-D Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import numpy as np\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "\n",
                "class CityscapesRGBDDataset(Dataset):\n",
                "    def __init__(self, images_base, labels_base, depth_base, split='train', \n",
                "                 num_classes=34, img_size=256):\n",
                "        self.images_base = images_base\n",
                "        self.labels_base = labels_base\n",
                "        self.depth_base = depth_base\n",
                "        self.split = split\n",
                "        self.num_classes = num_classes\n",
                "        self.img_size = img_size\n",
                "        \n",
                "        # Find all samples\n",
                "        self.samples = []\n",
                "        split_dir = os.path.join(images_base, split)\n",
                "        \n",
                "        if os.path.exists(split_dir):\n",
                "            for city in os.listdir(split_dir):\n",
                "                city_dir = os.path.join(split_dir, city)\n",
                "                if os.path.isdir(city_dir):\n",
                "                    for img_file in os.listdir(city_dir):\n",
                "                        if img_file.endswith('_leftImg8bit.png'):\n",
                "                            depth_file = img_file.replace('_leftImg8bit.png', '_disparity.png')\n",
                "                            depth_path = os.path.join(depth_base, split, city, depth_file)\n",
                "                            if os.path.exists(depth_path):\n",
                "                                self.samples.append((city, img_file))\n",
                "        \n",
                "        print(f\"  {split}: {len(self.samples)} RGB-D images\")\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        city, img_file = self.samples[idx]\n",
                "        try:\n",
                "            # Load RGB\n",
                "            img_path = os.path.join(self.images_base, self.split, city, img_file)\n",
                "            rgb = cv2.imread(img_path)\n",
                "            rgb = cv2.cvtColor(rgb, cv2.COLOR_BGR2RGB)\n",
                "            rgb = cv2.resize(rgb, (self.img_size, self.img_size))\n",
                "            \n",
                "            # Load Depth\n",
                "            depth_file = img_file.replace('_leftImg8bit.png', '_disparity.png')\n",
                "            depth_path = os.path.join(self.depth_base, self.split, city, depth_file)\n",
                "            depth = cv2.imread(depth_path, cv2.IMREAD_GRAYSCALE)\n",
                "            depth = cv2.resize(depth, (self.img_size, self.img_size))\n",
                "            \n",
                "            # Load Label\n",
                "            label_file = img_file.replace('_leftImg8bit.png', '_gtFine_labelIds.png')\n",
                "            label_path = os.path.join(self.labels_base, self.split, city, label_file)\n",
                "            mask = cv2.imread(label_path, cv2.IMREAD_GRAYSCALE)\n",
                "            mask = cv2.resize(mask, (self.img_size, self.img_size), \n",
                "                             interpolation=cv2.INTER_NEAREST)\n",
                "            \n",
                "            # Normalize and create RGBD\n",
                "            rgb = rgb.astype(np.float32) / 255.0\n",
                "            depth = depth.astype(np.float32) / 255.0\n",
                "            rgbd = np.concatenate([rgb, depth[:, :, np.newaxis]], axis=-1)\n",
                "            \n",
                "            # To tensors\n",
                "            rgbd = torch.from_numpy(rgbd).permute(2, 0, 1).float()\n",
                "            mask = torch.from_numpy(mask).long()\n",
                "            mask = torch.clamp(mask, 0, self.num_classes - 1)\n",
                "            \n",
                "            return rgbd, mask\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {img_file}: {e}\")\n",
                "            # Return zeros in case of error (robustness)\n",
                "            return torch.zeros(4, self.img_size, self.img_size), torch.zeros(self.img_size, self.img_size).long()\n""            "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets\n",
                "print(\"Loading datasets...\")\n",
                "train_dataset = CityscapesRGBDDataset(\n",
                "    images_base=config['images_base'],\n",
                "    labels_base=config['labels_base'],\n",
                "    depth_base=config['depth_base'],\n",
                "    split='train',\n",
                "    num_classes=config['num_classes'],\n",
                "    img_size=config['img_size']\n",
                ")\n",
                "\n",
                "val_dataset = CityscapesRGBDDataset(\n",
                "    images_base=config['images_base'],\n",
                "    labels_base=config['labels_base'],\n",
                "    depth_base=config['depth_base'],\n",
                "    split='val',\n",
                "    num_classes=config['num_classes'],\n",
                "    img_size=config['img_size']\n",
                ")\n",
                "\n",
                "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], \n",
                "                         shuffle=True, num_workers=config['num_workers'])\n",
                "val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], \n",
                "                       shuffle=False, num_workers=config['num_workers'])\n",
                "\n",
                "print(f\"\\nTrain: {len(train_dataset)} samples, {len(train_loader)} batches\")\n",
                "print(f\"Val: {len(val_dataset)} samples, {len(val_loader)} batches\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Train Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.notebook import tqdm\n",
                "\n",
                "# Initialize model\n",
                "model = UNetRGBD(in_channels=4, num_classes=34, base_channels=config['base_channels']).to(config['device'])\n",
                "criterion = nn.CrossEntropyLoss()\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
                "\n",
                "print(f\"Model: {model.get_num_params():,} parameters\")\n",
                "print(f\"Device: {config['device']}\")\n",
                "\n",
                "# Training loop\n",
                "best_val_loss = float('inf')\n",
                "history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
                "\n",
                "for epoch in range(config['num_epochs']):\n",
                "    # Training\n",
                "    model.train()\n",
                "    train_loss = 0\n",
                "    \n",
                "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Train]\")\n",
                "    for rgbd, mask in pbar:\n",
                "        rgbd, mask = rgbd.to(config['device']), mask.to(config['device'])\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        output = model(rgbd)\n",
                "        loss = criterion(output, mask)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        train_loss += loss.item()\n",
                "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
                "    \n",
                "    train_loss /= len(train_loader)\n",
                "    \n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0\n",
                "    correct = 0\n",
                "    total = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for rgbd, mask in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']} [Val]\"):\n",
                "            rgbd, mask = rgbd.to(config['device']), mask.to(config['device'])\n",
                "            output = model(rgbd)\n",
                "            loss = criterion(output, mask)\n",
                "            val_loss += loss.item()\n",
                "            \n",
                "            pred = output.argmax(dim=1)\n",
                "            correct += (pred == mask).sum().item()\n",
                "            total += mask.numel()\n",
                "    \n",
                "    val_loss /= len(val_loader)\n",
                "    val_acc = correct / total\n",
                "    \n",
                "    # Save history\n",
                "    history['train_loss'].append(train_loss)\n",
                "    history['val_loss'].append(val_loss)\n",
                "    history['val_acc'].append(val_acc)\n",
                "    \n",
                "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
                "    \n",
                "    # Save best model\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        torch.save({\n",
                "            'epoch': epoch + 1,\n",
                "            'model_state_dict': model.state_dict(),\n",
                "            'optimizer_state_dict': optimizer.state_dict(),\n",
                "            'loss': val_loss,\n",
                "            'accuracy': val_acc,\n",
                "            'config': config\n",
                "        }, 'best_rgbd_model.pth')\n",
                "        print(\"  âœ“ Best model saved!\")\n",
                "\n",
                "print(\"\\nâœ“ Training completed!\")\n",
                "print(f\"Best validation loss: {best_val_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Plot Training Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
                "\n",
                "# Loss\n",
                "ax1.plot(history['train_loss'], label='Train Loss')\n",
                "ax1.plot(history['val_loss'], label='Val Loss')\n",
                "ax1.set_xlabel('Epoch')\n",
                "ax1.set_ylabel('Loss')\n",
                "ax1.set_title('Training and Validation Loss')\n",
                "ax1.legend()\n",
                "ax1.grid(True)\n",
                "\n",
                "# Accuracy\n",
                "ax2.plot(history['val_acc'], label='Val Accuracy', color='green')\n",
                "ax2.set_xlabel('Epoch')\n",
                "ax2.set_ylabel('Pixel Accuracy')\n",
                "ax2.set_title('Validation Accuracy')\n",
                "ax2.legend()\n",
                "ax2.grid(True)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_curves.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"âœ“ Training curves saved to training_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Download Trained Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "# Download model\n",
                "print(\"Downloading trained model...\")\n",
                "files.download('best_rgbd_model.pth')\n",
                "\n",
                "# Download training curves\n",
                "print(\"Downloading training curves...\")\n",
                "files.download('training_curves.png')\n",
                "\n",
                "print(\"\\nâœ“ Downloads complete!\")\n",
                "print(\"\\nYou can now use this model for inference on your local machine.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Model trained successfully!**\n",
                "\n",
                "- Model: U-Net with RGB-D input (4 channels)\n",
                "- Dataset: Cityscapes (RGB + Disparity)\n",
                "- Training device: GPU\n",
                "- Downloaded: `best_rgbd_model.pth`\n",
                "\n",
                "**Next steps:**\n",
                "1. Transfer `best_rgbd_model.pth` to your local machine\n",
                "2. Run inference using `run_inference_rgbd.py`"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}